{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "pm2dJFf115Ym",
        "D5kw4io14S7h"
      ],
      "authorship_tag": "ABX9TyNsDPOavrm2d9saZv+AIlZd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZainaHweij/csdiagnostic/blob/main/Test_Trained_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test ACS intracompartmental pressure model using data from sheet\n"
      ],
      "metadata": {
        "id": "RPLxna0ptVwd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate predicted pressure"
      ],
      "metadata": {
        "id": "efeNoe8Q549h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mount Google Drive"
      ],
      "metadata": {
        "id": "0Yvbce0otR6L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "baUOdUoCslqu"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test model using CSV data sheet"
      ],
      "metadata": {
        "id": "eTJTetuFtsYI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load the TensorFlow Lite model\n",
        "model_path = '/content/drive/My Drive/SEF 2023-2024/TinyML/model.tflite'\n",
        "interpreter = tf.lite.Interpreter(model_path=model_path)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Load data from a CSV file\n",
        "csv_path = '/content/drive/MyDrive/SEF 2023-2024/TinyML/fsr_data.csv'\n",
        "data = pd.read_csv(csv_path)\n",
        "\n",
        "# Create an empty list to store predicted values\n",
        "predicted_values = []\n",
        "\n",
        "for index, row in data.iterrows():\n",
        "    input_features = row[['fsr_1', 'fsr_2', 'fsr_3', 'fsr_4', 'fsr_5', 'fsr_6', 'gyro']].values.astype(np.float32)\n",
        "\n",
        "    # Set input tensor\n",
        "    input_details = interpreter.get_input_details()\n",
        "    interpreter.set_tensor(input_details[0]['index'], input_features.reshape(1, -1))\n",
        "\n",
        "    # Run inference\n",
        "    interpreter.invoke()\n",
        "\n",
        "    # Get output tensor\n",
        "    output_details = interpreter.get_output_details()\n",
        "    output_data = interpreter.get_tensor(output_details[0]['index']).flatten()  # Flatten the output_data\n",
        "\n",
        "    # Append the flattened predicted value to the list\n",
        "    predicted_values.append(output_data)\n",
        "\n",
        "# Convert the list to a numpy array for easier manipulation\n",
        "predicted_values = np.array(predicted_values)\n",
        "print(\"Predicted values generated.\")"
      ],
      "metadata": {
        "id": "KO0zHid1twlA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print all output data if needed"
      ],
      "metadata": {
        "id": "K9s3ePo81ZCd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for index, row in data.iterrows():\n",
        "    # Process the output data as needed\n",
        "    print(f\"Row {index + 1} - Input Data: {input_features}\")\n",
        "    print(f\"Row {index + 1} - Output Data: {output_data}\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "# Print all predicted values in array\n",
        "print(\"All Predicted Values:\")\n",
        "print(predicted_values)"
      ],
      "metadata": {
        "id": "yuRRArgu1JgK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate confusion matrix and related performance metrics"
      ],
      "metadata": {
        "id": "pm2dJFf115Ym"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "threshold = 30\n",
        "\n",
        "# Convert ground truth to binary labels (1 if disease, 0 if no disease)\n",
        "ground_truth_labels = (data['ground_truth'] > threshold).astype(int)\n",
        "\n",
        "# Convert predicted values to binary labels (1 if predicted disease, 0 if predicted no disease)\n",
        "predicted_labels = (predicted_values > threshold).astype(int)\n",
        "\n",
        "# Calculate accuracy using sklearn's accuracy_score\n",
        "accuracy = accuracy_score(ground_truth_labels, predicted_labels)\n",
        "\n",
        "# Calculate confusion matrix\n",
        "conf_matrix = confusion_matrix(ground_truth_labels, predicted_labels)\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "# Extract TP, TN, FP, FN from the confusion matrix\n",
        "tn, fp, fn, tp = conf_matrix.ravel()\n",
        "\n",
        "# Calculate sensitivity (recall), specificity, precision, and F1 score\n",
        "sensitivity = tp / (tp + fn) if (tp + fn) != 0 else 0  # Check for division by zero\n",
        "specificity = tn / (tn + fp) if (tn + fp) != 0 else 0  # Check for division by zero\n",
        "precision = tp / (tp + fp) if (tp + fp) != 0 else 0  # Check for division by zero\n",
        "f1_score = 2 * (precision * sensitivity) / (precision + sensitivity) if (precision + sensitivity) != 0 else 0  # Check for division by zero\n",
        "\n",
        "print(f\"\\nAccuracy: {accuracy}\")\n",
        "print(f\"Sensitivity (Recall): {sensitivity}\")\n",
        "print(f\"Specificity: {specificity}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"F1 Score: {f1_score}\")"
      ],
      "metadata": {
        "id": "UhaMFlSDxtXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize Data"
      ],
      "metadata": {
        "id": "D5kw4io14S7h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install matplotlib seaborn"
      ],
      "metadata": {
        "id": "02ZDdTXa4l2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize Confusion Matrix (heat map)"
      ],
      "metadata": {
        "id": "kWkAjJp16sYi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Visualize Confusion Matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
        "            xticklabels=['Predicted Negative (0)', 'Predicted Positive (1)'],\n",
        "            yticklabels=['Actual Negative (0)', 'Actual Positive (1)'])\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted ACS')\n",
        "plt.ylabel('True ACS')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XwCzlfyv4Ucn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize Metrics (bar chart)"
      ],
      "metadata": {
        "id": "hhdPxEvr6884"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_names = ['Accuracy', 'Sensitivity (Recall)', 'Specificity', 'Precision', 'F1 Score']\n",
        "metrics_values = [accuracy, sensitivity, specificity, precision, f1_score]\n",
        "\n",
        "plt.figure(figsize=(10, 5)) # Higher = better\n",
        "plt.bar(metrics_names, metrics_values, color=['skyblue', 'lightgreen', 'lightcoral', 'gold', 'orchid']) # Can select other colors\n",
        "plt.title('Model Performance Metrics')\n",
        "plt.ylabel('Score')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EDHPieKV6OS1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}